{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "device = 'mps' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# downloading mnist\n",
    "mnist_trainset = \n",
    "mnist_testset = \n",
    "\n",
    "## printing lengths\n",
    "\n",
    "print('length of the training set: {}'.format(len(mnist_trainset)))\n",
    "print('length of the test set: {}'.format(len(mnist_testset)))\n",
    "\n",
    "## rendering a few example from each label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "partition_index = \n",
    "\n",
    "def one_hot(y):\n",
    "  #For converting a numpy array of 0-9 into a one hot encoding of vectors of length 10\n",
    "  b = np.zeros((y.size, y.max() + 1))\n",
    "  b[np.arange(y.size), y] = 1\n",
    "  return b\n",
    "\n",
    "print('processing labeld training x and y')\n",
    "train_x = np.asarray([np.asarray() for i in tqdm(range())])\n",
    "train_y = one_hot(np.asarray([np.asarray() for i in tqdm(range())]))\n",
    "\n",
    "print('processing unlabled training data')\n",
    "train_unlabled = np.asarray([np.asarray() for i in tqdm(range(,len()))])\n",
    "\n",
    "print('processing labeld test x and y')\n",
    "test_x = np.asarray([np.asarray() for i in tqdm(range(len()))])\n",
    "test_y = one_hot(np.asarray([np.asarray() for i in tqdm(range(len()))]))\n",
    "\n",
    "print('reformatting shape...')\n",
    "train_x = np.expand_dims(train_x, 1)\n",
    "train_unlabled = np.expand_dims(train_unlabled, 1)\n",
    "test_x = np.expand_dims(test_x, 1)\n",
    "\n",
    "#converting data to pytorch type\n",
    "torch_train_x = torch.tensor(, requires_grad=True).to(device)\n",
    "torch_train_y = torch.tensor().to(device)\n",
    "torch_test_x = torch.tensor(test_x.astype(np.float32), requires_grad=True).to(device)\n",
    "torch_test_y = torch.tensor(test_y).to(device)\n",
    "torch_train_unlabled = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Backbone, self).__init__()\n",
    "        self.conv1 = nn.Conv2d()\n",
    "        self.conv2 = nn.Conv2d()\n",
    "        self.conv3 = nn.Conv2d()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(), 2)\n",
    "        x = F.max_pool2d(F.relu(), 2)\n",
    "        x = F.max_pool2d(F.relu(), 2)\n",
    "        x = torch.flatten(, 1)\n",
    "        return x\n",
    "\n",
    "#defining model head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_class=10):\n",
    "        super(Head, self).__init__()\n",
    "        self.fc1 = nn.Linear(, )\n",
    "        self.fc2 = nn.Linear(, )\n",
    "        self.fc3 = nn.Linear(, )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu()\n",
    "        x = F.relu()\n",
    "        x = \n",
    "        return \n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.backbone = \n",
    "        self.head = \n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "model_baseline = \n",
    "print(model_baseline(torch_train_x[:1]).shape)\n",
    "model_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train without unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_train(model):\n",
    "\n",
    "    batch_size = \n",
    "    lr = \n",
    "    momentum = # set to 0.9 or 0.95\n",
    "    num_epochs = # a value more than 2000\n",
    "\n",
    "    #defining a stocastic gradient descent optimizer\n",
    "    optimizer = torch.optim.SGD()\n",
    "\n",
    "    #defining loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_hist = []\n",
    "    test_hist = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        #iterating over all batches\n",
    "        for i in range(int(len(train_x)/batch_size)-1):\n",
    "\n",
    "            #Put the model in training mode, so that things like dropout work\n",
    "            \n",
    "\n",
    "            # Zero gradients for the optimizer \n",
    "            \n",
    "\n",
    "            #extracting X and y values from the batch\n",
    "            X = torch_train_x[i*batch_size: (i+1)*batch_size]\n",
    "            y = torch_train_y[i*batch_size: (i+1)*batch_size]\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            y_pred = \n",
    "\n",
    "            #compute gradients with the loss function\n",
    "            \n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #Disable things like dropout, if they exist\n",
    "            model.train(False)\n",
    "\n",
    "            #calculating epoch training and test loss\n",
    "            train_loss = loss_fn(model(torch_train_x), torch_train_y).cpu().numpy()\n",
    "            y_pred_test = model(torch_test_x)\n",
    "            test_loss = loss_fn(y_pred_test, torch_test_y).cpu().numpy()\n",
    "\n",
    "            train_hist.append() # use train loss to plot\n",
    "            test_hist.append() # use test loss to plot\n",
    "\n",
    "            #computing test accuracy\n",
    "            matches = np.equal(np.argmax(y_pred_test.cpu().numpy(), axis=1), np.argmax(torch_test_y.cpu().numpy(), axis=1))\n",
    "            test_accuracy.append()\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_hist, label = 'train loss')\n",
    "    plt.plot(test_hist, label = 'test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(test_accuracy, label = 'test accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    maxacc = # maximum value of accuracy\n",
    "    print('max accuracy: {}'.format(maxacc))\n",
    "    \n",
    "    return maxacc\n",
    "\n",
    "supervised_maxacc = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Augment:\n",
    "\n",
    "   def __init__(self):\n",
    "\n",
    "       blur = T.GaussianBlur()\n",
    "\n",
    "       self.train_transform = torch.nn.Sequential(\n",
    "           T.RandomAffine(),\n",
    "           T.RandomPerspective(),\n",
    "           T.RandomPerspective(),\n",
    "           T.RandomPerspective(0.2,0.5),\n",
    "           T.RandomApply([blur],),\n",
    "           T.RandomApply([blur],)\n",
    "       )\n",
    "\n",
    "   def __call__(self, x):\n",
    "       return self.train_transform(x), self.train_transform(x)\n",
    "\n",
    "\"\"\"\n",
    "Generating Test Augmentation\n",
    "\"\"\"\n",
    "a = Augment()\n",
    "aug = a(torch_train_unlabled[0:100])\n",
    "\n",
    "i=1\n",
    "f, axarr = plt.subplots(2,2)\n",
    "#positive pair\n",
    "axarr[0,0].imshow(aug[0].cpu().detach().numpy()[i,0])\n",
    "axarr[0,1].imshow(aug[1].cpu().detach().numpy()[i,0])\n",
    "#another positive pair\n",
    "axarr[1,0].imshow(aug[0].cpu().detach().numpy()[i+1,0])\n",
    "axarr[1,1].imshow(aug[1].cpu().detach().numpy()[i+1,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "   def __init__(self, batch_size, temperature=0.5):\n",
    "\n",
    "       super().__init__()\n",
    "       self.batch_size = batch_size\n",
    "       self.temperature = temperature\n",
    "       self.mask = (~torch.eye(batch_size * 2, batch_size * 2, dtype=bool)).float().to(device)\n",
    "\n",
    "   def calc_similarity_batch(self, a, b):\n",
    "       representations = torch.cat([a, b], dim=0)\n",
    "       return F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "\n",
    "   def forward(self, proj_1, proj_2):\n",
    "       batch_size = proj_1.shape[0]\n",
    "       z_i = F.normalize(proj_1, p=2, dim=1)\n",
    "       z_j = F.normalize(proj_2, p=2, dim=1)\n",
    "\n",
    "       similarity_matrix = self.calc_similarity_batch(z_i, z_j)\n",
    "\n",
    "       sim_ij = torch.diag(similarity_matrix, batch_size)\n",
    "       sim_ji = torch.diag(similarity_matrix, -batch_size)\n",
    "\n",
    "       positives = torch.cat([sim_ij, sim_ji], dim=0)\n",
    "\n",
    "       nominator = torch.exp(positives / self.temperature)\n",
    "\n",
    "       denominator = self.mask * torch.exp(similarity_matrix / self.temperature)\n",
    "\n",
    "       all_losses = -torch.log(nominator / torch.sum(denominator, dim=1))\n",
    "       loss = torch.sum(all_losses) / (2 * self.batch_size)\n",
    "       return loss\n",
    "\n",
    "loss = # use the above loss\n",
    "fake_proj_0, fake_proj_1 = a(torch_train_x)\n",
    "fake_proj_0 = fake_proj_0[:,0,:,0]\n",
    "fake_proj_1 = fake_proj_1[:,0,:,0]\n",
    "loss(fake_proj_0, fake_proj_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "model = \n",
    "model.train()\n",
    "\n",
    "#defining key hyperparameters\n",
    "batch_size = 512\n",
    "epoch_size = \n",
    "num_epochs = # more than 50\n",
    "patience = 5\n",
    "cutoff_ratio = 0.001\n",
    "\n",
    "#defining key learning functions\n",
    "optimizer = torch.optim.Adam(, lr=1e-3)\n",
    "num_examples = \n",
    "lossfn = \n",
    "augmentfn = \n",
    "\n",
    "#for book keeping\n",
    "loss_hist = []\n",
    "improvement_hist = []\n",
    "schedule_hist = []\n",
    "\n",
    "#for exponentially decreasing learning rate\n",
    "scheduler = ExponentialLR(, gamma = 0.95)\n",
    "\n",
    "#for early stopping\n",
    "patience_count = 0\n",
    "\n",
    "#Training Loop\n",
    "avg_loss = 1e10\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    print('epoch {}/{}'.format(i,num_epochs))\n",
    "\n",
    "    total_loss = 0\n",
    "    loss_change = 0\n",
    "\n",
    "    for j in tqdm(range(epoch_size)):\n",
    "\n",
    "        #getting random batch\n",
    "        X = torch_train_unlabled[j*batch_size: (j+1)*batch_size]\n",
    "\n",
    "        #creating pairs of augmented batches\n",
    "        X_aug_i, X_aug_j = \n",
    "\n",
    "        #ensuring gradients are zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #passing through the model\n",
    "        z_i = model()\n",
    "        z_j = model()\n",
    "\n",
    "        #calculating loss on the model embeddings, and computing gradients\n",
    "        loss = \n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if True:\n",
    "            z_i = model(X_aug_i)\n",
    "            z_j = model(X_aug_j)\n",
    "\n",
    "            #calculating new loss value\n",
    "            new_loss = lossfn(z_i, z_j)\n",
    "\n",
    "            loss_change += new_loss.cpu().detach().numpy() - loss.cpu().detach().numpy()\n",
    "\n",
    "        total_loss += \n",
    "\n",
    "        #step learning rate scheduler\n",
    "        schedule_hist.append(scheduler.get_last_lr())\n",
    "\n",
    "    #########################\n",
    "    # update scheduler here #\n",
    "    #########################\n",
    "    \n",
    "    #calculating percentage loss reduction\n",
    "    new_avg_loss = total_loss/epoch_size\n",
    "    per_loss_reduction = (avg_loss-new_avg_loss)/avg_loss\n",
    "    print('Percentage Loss Reduction: {}'.format(per_loss_reduction))\n",
    "\n",
    "    #deciding to stop if loss is not decreasing fast enough\n",
    "    if per_loss_reduction < cutoff_ratio:\n",
    "        patience_count+=1\n",
    "        print('patience counter: {}'.format(patience_count))\n",
    "        if patience_count > patience:\n",
    "            break\n",
    "    else:\n",
    "        patience_count = 0\n",
    "\n",
    "    #setting new loss as previous loss\n",
    "    avg_loss = new_avg_loss\n",
    "\n",
    "    #book keeping\n",
    "    avg_improvement = loss_change/epoch_size\n",
    "    loss_hist.append(avg_loss)\n",
    "    improvement_hist.append(avg_improvement)\n",
    "    print('Average Loss: {}'.format(avg_loss))\n",
    "    print('Average Loss change (if calculated): {}'.format(avg_improvement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(schedule_hist, label='learning rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(loss_hist, label = 'loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
